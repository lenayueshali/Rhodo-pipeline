# -*- coding: utf-8 -*-
"""DTW pipeline_LL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MGY4d1FqszXCmatpxykgtHqAzxqSnhts
"""

# --- INSTALLS (Colab) ---
!pip install --quiet dtaidistance

# --- IMPORTS ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns

from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

from dtaidistance import dtw
import gspread
from google.colab import auth, drive
from google.auth import default

import warnings
import logging
import re

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)

# --- GLOBAL CONFIGURATION ---
CONFIG = {
    'paths': {
        'wally_csv': "/content/drive/MyDrive/PhD data/Environmental data/wally_hourly_reef_flat_temps.csv"
    },
    'sheets': {
        'laser_workbook': 'Laser transects',
    },
    'dates': {
        'deploy_start': '2024-03-16',
        'deploy_end':   '2024-07-23',
        'calib_start':  '2024-04-16'
    },
    'dtw': {
        'psi': 0
    },
    'min_spots': 10
}

COLORS = {'Mg/Sr': 'magenta', 'Mg/Ca': 'green', 'Sr/Ca': 'tab:blue'}

# --- HELPER FUNCTIONS (standalone) ---

def _find_col(df, includes_all=(), includes_any=()):
    cols = list(df.columns)
    def ok(c):
        c0 = c.lower().replace(" ", "").split('(')[0]
        return all(s.lower().replace(" ", "") in c0 for s in includes_all) and \
               (any(s.lower().replace(" ", "") in c0 for s in includes_any) if includes_any else True)
    hits = [c for c in cols if ok(c)]
    return next((h for h in hits if ".1" not in h), hits[0] if hits else None)

def _reg_stats(x, y, name=""):
    x, y = np.asarray(x), np.asarray(y)
    m = np.isfinite(x) & np.isfinite(y)
    if m.sum() < 3:
        return dict(r=np.nan, r2=np.nan, p=np.nan, rmse=np.nan, n=0)
    r, p = pearsonr(x[m], y[m])
    model = LinearRegression().fit(x[m].reshape(-1,1), y[m])
    rmse = np.sqrt(mean_squared_error(y[m], model.predict(x[m].reshape(-1,1))))
    logging.info(f"{name}: R={r:.3f}, R²={r**2:.3f}, p={p:.3f}, RMSE={rmse:.3f}°C (n={m.sum()})")
    return dict(r=float(r), r2=float(r**2), p=float(p), rmse=float(rmse), n=int(m.sum()))

class RhodolithPipeline:
    """
    Rhodolith Temperature Proxy Pipeline using Mg/Sr-based DTW alignment.
    """

    def __init__(self, config):
        self.config = config
        self.gc = None

        # Environmental
        self.temp_full = None
        self.temp_max = None
        self.full_series = None

        # Screening
        self.best_branch_meta = {}

        # Synthetic master
        self.synthetic_master = None

        # Alignment
        self.aligned_data = {}
        self.dtw_diagnostics = []

        # Composite & calibration
        self.composite_data = {}
        self.final_equations = {}

        # Diagnostics
        self.bootstrap_results = {}
        self.leave_one_out_stats = None

        # Curve6/7
        self.curve6 = None
        self.curve7 = None

    # ---------- STEP 1: AUTH ----------
    def authenticate(self):
        print("="*60)
        print("STEP 1: AUTHENTICATION")
        print("="*60)
        drive.mount('/content/drive', force_remount=True)
        auth.authenticate_user()
        creds, _ = default()
        self.gc = gspread.authorize(creds)
        print("✓ Authenticated.\n")

    # ---------- STEP 2: TEMPERATURE ----------
    def load_temperature_data(self):
        print("="*60)
        print("STEP 2: LOAD TEMPERATURE")
        print("="*60)

        df = pd.read_csv(self.config['paths']['wally_csv'], parse_dates=['Index'])
        mask = (df['Index'] >= self.config['dates']['deploy_start']) & \
               (df['Index'] <= self.config['dates']['deploy_end'])
        full_series = df.loc[mask].set_index('Index')['temp.exposed.af']

        self.full_series = full_series
        self.temp_full = full_series.resample('D').mean().interpolate('time')
        self.temp_max  = full_series.resample('D').max().interpolate('time')
        self.temp_min = full_series.resample('D').min().interpolate(method='time')

        print(f"✓ Temp loaded: {len(self.temp_full)} days.")
        print(f"  Range: {self.temp_full.min():.2f}–{self.temp_full.max():.2f} °C\n")

    # ---------- UTIL: LOAD BRANCH ----------
    def get_raw_branch_data(self, branch_num):
        target_name_lower = f"afe5-{branch_num}".lower()
        try:
            wb = self.gc.open(self.config['sheets']['laser_workbook'])
            found_ws = None
            for ws in wb.worksheets():
                if target_name_lower in ws.title.lower().replace(" ", ""):
                    found_ws = ws
                    break
            if found_ws:
                raw = found_ws.get_all_values()
                if len(raw) > 1:
                    df = pd.DataFrame(raw[1:], columns=raw[0])
                    df = df.loc[:, ~df.columns.duplicated(keep='first')]
                    clean_name = re.sub(r'(?i)copy of|raw', '', found_ws.title).strip()
                    return df, clean_name
        except Exception as e:
            logging.error(f"Error accessing branch {branch_num}: {e}")
        return None, None

    # ---------- CURVE6 & CURVE7 LOAD ----------
    def load_curve6_curve7(self):
        print("="*60)
        print("LOAD CURVE6 & CURVE7 (Rhodo25_data)")
        print("="*60)
        try:
            wb = self.gc.open('Rhodo25_data')
        except Exception as e:
            print(f"❌ Could not open Rhodo25_data: {e}")
            self.curve6 = None
            self.curve7 = None
            return

        def _load_curve(sheet_name):
            try:
                ws = wb.worksheet(sheet_name)
                raw = ws.get_all_values()
                headers = [h.strip() for h in raw[0]]
                data = []
                for row in raw[1:]:
                    row_data = {}
                    for i, h in enumerate(headers):
                        if i < len(row):
                            row_data[h] = row[i]
                    data.append(row_data)
                df = pd.DataFrame(data)
                df['DateTime'] = pd.to_datetime(df['DateTime'], errors='coerce')
                df = df.dropna(subset=['DateTime']).sort_values('DateTime')
                return df
            except Exception as e:
                print(f"  ⚠ Error loading '{sheet_name}': {e}")
                return None

        self.curve6 = _load_curve('curve6')
        self.curve7 = _load_curve('curve7')

        if self.curve6 is not None:
            print(f"✓ Curve6 loaded ({len(self.curve6)} increments)")
        if self.curve7 is not None:
            print(f"✓ Curve7 loaded ({len(self.curve7)} increments)")
        print()

    # ---------- HELPER: TIME AXIS FROM CURVE ----------
    def _time_axis_from_curve(self, n_points, curve_df):
        curve_times = curve_df['DateTime'].dropna().sort_values().values
        if len(curve_times) < 2:
            t_start = pd.to_datetime(self.config['dates']['deploy_start'])
            t_end   = pd.to_datetime(self.config['dates']['deploy_end'])
            return pd.date_range(start=t_start, end=t_end, periods=n_points)

        t_param = np.linspace(0, 1, len(curve_times))
        t_new   = np.linspace(0, 1, n_points)
        ordinals = curve_times.astype('datetime64[ns]').astype('int64')
        ord_interp = np.interp(t_new, t_param, ordinals)
        return pd.to_datetime(ord_interp)

    # ---------- STEP 3: SCREENING ----------
    def screen_best_linear_branch(self):
        """
        Linear models for all branches + Curve6/Curve7 time models for AFE5-1.
        Winner defines Mg/Sr = m*Temp + c for the synthetic master.
        """
        print("="*60)
        print("STEP 3: SCREENING (Linear + Curve6/7 for AFE5-1)")
        print("="*60)

        t_start = pd.to_datetime(self.config['dates']['deploy_start'])
        t_end   = pd.to_datetime(self.config['dates']['deploy_end'])

        best_r2 = -np.inf
        best_branch = None
        best_model_type = None
        best_params = {}

        for i in range(1, 8):
            df_raw, name = self.get_raw_branch_data(i)
            if df_raw is None:
                continue

            df = df_raw.apply(pd.to_numeric, errors='coerce')
            sr_col = _find_col(df, ('ca',), ('sr',))
            mg_col = _find_col(df, ('ca',), ('mg',))
            if not sr_col or not mg_col:
                continue

            mgsr = df[mg_col] / df[sr_col]
            valid = mgsr.dropna()
            if len(valid) < self.config['min_spots']:
                continue

            y = valid.values

            # (A) Linear time axis
            linear_dates = pd.date_range(start=t_start, end=t_end, periods=len(valid))
            temps_lin = self.temp_full.reindex(linear_dates, method='nearest').values
            X_lin = temps_lin.reshape(-1, 1)
            mask_lin = np.isfinite(X_lin.ravel()) & np.isfinite(y)
            r2_lin = np.nan
            if mask_lin.sum() >= 10:
                model_lin = LinearRegression().fit(X_lin[mask_lin], y[mask_lin])
                r2_lin = model_lin.score(X_lin[mask_lin], y[mask_lin])
                slope_lin = float(model_lin.coef_[0])
                intercept_lin = float(model_lin.intercept_)
            print(f"  {name}: Linear R² = {r2_lin:.4f}")

            if r2_lin > best_r2:
                best_r2 = r2_lin
                best_branch = name
                best_model_type = 'linear'
                best_params = {'slope': slope_lin, 'intercept': intercept_lin}

            # (B) Curve6/7 time for AFE5-1
            if i == 1:
                for mtype, curve_attr in [('curve6', 'curve6'), ('curve7', 'curve7')]:
                    curve_df = getattr(self, curve_attr, None)
                    if curve_df is None:
                        continue
                    dates_curve = self._time_axis_from_curve(len(valid), curve_df)
                    temps_curve = self.temp_full.reindex(dates_curve, method='nearest').values
                    X_curve = temps_curve.reshape(-1, 1)
                    mask_curve = np.isfinite(X_curve.ravel()) & np.isfinite(y)
                    r2_curve = np.nan
                    if mask_curve.sum() >= 10:
                        model_curve = LinearRegression().fit(X_curve[mask_curve], y[mask_curve])
                        r2_curve = model_curve.score(X_curve[mask_curve], y[mask_curve])
                        slope_curve = float(model_curve.coef_[0])
                        intercept_curve = float(model_curve.intercept_)
                    print(f"      {mtype} time R² = {r2_curve:.4f}")

                    if r2_curve > best_r2:
                        best_r2 = r2_curve
                        best_branch = name
                        best_model_type = mtype
                        best_params = {'slope': slope_curve, 'intercept': intercept_curve}

        if best_branch is None:
            print("❌ Screening failed (no valid branches).\n")
            return

        self.best_branch_meta = best_params
        self.best_branch_meta['name'] = best_branch
        self.best_branch_meta['model_type'] = best_model_type

        print(f"\n✓ WINNER: {best_branch} using {best_model_type} time axis")
        print(f"  Mg/Sr = {best_params['slope']:.4f} * Temp + {best_params['intercept']:.4f}")
        print(f"  Best R² = {best_r2:.4f}\n")
    # ---------- STEP 4: SYNTHETIC MASTER ----------
    def generate_synthetic_master(self):
        print("="*60)
        print("STEP 4: GENERATE SYNTHETIC Mg/Sr MASTER")
        print("="*60)

        if not self.best_branch_meta:
            print("❌ Run screen_best_linear_branch() first.\n")
            return

        m = self.best_branch_meta['slope']
        c = self.best_branch_meta['intercept']
        pred_mgsr = m * self.temp_full + c
        self.synthetic_master = pd.DataFrame({
            'Date': self.temp_full.index,
            'MgSr_Target': pred_mgsr
        })

        print(f"✓ Synthetic Master Created ({len(pred_mgsr)} days)")
        print(f"  Mg/Sr range: {pred_mgsr.min():.2f}–{pred_mgsr.max():.2f} mmol/mol\n")

    # ---------- STEP 5: DTW ALIGNMENT ----------
    def perform_dtw_alignment(self, window_days=15, label_suffix=""):
        print("="*60)
        print(f"STEP 5: UNIFIED DTW ALIGNMENT (window={window_days} days)")
        print("="*60)

        if self.synthetic_master is None:
            print("❌ Synthetic master missing; run generate_synthetic_master() first.\n")
            return

        master_vals = self.synthetic_master['MgSr_Target'].values
        master_dates = self.synthetic_master['Date'].values
        window = int(window_days) if window_days is not None else None

        aligned_mgsr_list = []
        aligned_mgca_list = []
        aligned_srca_list = []
        self.dtw_diagnostics = []

        for i in range(1, 8):
            df_raw, name = self.get_raw_branch_data(i)
            if df_raw is None:
                continue

            df = df_raw.apply(pd.to_numeric, errors='coerce')
            sr_col = _find_col(df, ('ca',), ('sr',))
            mg_col = _find_col(df, ('ca',), ('mg',))
            if not sr_col or not mg_col:
                logging.warning(f"{name}: missing Mg or Sr column, skipping")
                continue

            mg = df[mg_col]
            sr = df[sr_col]
            mgsr = mg / sr

            valid_mask = mgsr.notna() & mg.notna() & sr.notna()
            if valid_mask.sum() < self.config['min_spots']:
                logging.warning(f"{name}: only {valid_mask.sum()} valid points, skipping")
                continue

            mgsr_clean = mgsr[valid_mask].values
            mg_clean   = mg[valid_mask].values
            sr_clean   = sr[valid_mask].values

            try:
                path = dtw.warping_path(mgsr_clean, master_vals,
                                        window=window,
                                        psi=self.config['dtw']['psi'])

                n_q = len(mgsr_clean)
                n_m = len(master_vals)
                stretch = len(path) / max(n_q, n_m)
                self.dtw_diagnostics.append({
                    'branch': name,
                    'n_query': n_q,
                    'n_master': n_m,
                    'path_len': len(path),
                    'stretch_factor': stretch
                })

                q_idx = [p[0] for p in path]
                m_idx = [p[1] for p in path]
                dates = master_dates[m_idx]

                aligned_mgsr_list.append(pd.DataFrame({'Date': dates, 'Branch': name, 'Value': mgsr_clean[q_idx]}))
                aligned_mgca_list.append(pd.DataFrame({'Date': dates, 'Branch': name, 'Value': mg_clean[q_idx]}))
                aligned_srca_list.append(pd.DataFrame({'Date': dates, 'Branch': name, 'Value': sr_clean[q_idx]}))

                print(f"  ✓ Aligned {name}: {len(path)} steps, stretch={stretch:.2f}")
            except Exception as e:
                logging.error(f"DTW failed for {name}: {e}")

        if not aligned_mgsr_list:
            print("❌ No branches aligned; check DTW settings.\n")
            return

        self.aligned_data['Mg/Sr'] = pd.concat(aligned_mgsr_list, ignore_index=True)
        self.aligned_data['Mg/Ca'] = pd.concat(aligned_mgca_list, ignore_index=True)
        self.aligned_data['Sr/Ca'] = pd.concat(aligned_srca_list, ignore_index=True)

        diag_df = pd.DataFrame(self.dtw_diagnostics)
        print("\nDTW stretch factors:")
        print(diag_df[['branch', 'stretch_factor']].to_string(index=False))
        print(f"\nMean stretch factor: {diag_df['stretch_factor'].mean():.2f}\n")

    # ---------- STEP 6: COMPOSITE & CALIBRATION ----------
    def build_composite_and_calibrate(self):
        print("="*60)
        print("STEP 6: COMPOSITE & FINAL CALIBRATION")
        print("="*60)

        # Use full deployment window for composites and calibration
        calib_start = pd.to_datetime(self.config['dates']['deploy_start'])

        self.composite_data = {}
        self.final_equations = {}

        for base in ['Mg/Sr', 'Mg/Ca', 'Sr/Ca']:
            if base not in self.aligned_data:
                continue

            df = self.aligned_data[base]
            df = df[df['Date'] >= calib_start]

            comp = df.groupby('Date')['Value'].agg(['mean', 'std', 'count']).sort_index()
            self.composite_data[base] = comp

            common_dates = comp.index.intersection(self.temp_full.index)
            if len(common_dates) < 10:
                print(f"  ⚠ {base}: too few overlapping days for calibration.")
                continue

            x_proxy = comp.loc[common_dates, 'mean'].values
            y_temp = self.temp_full.loc[common_dates].values
            mask = np.isfinite(x_proxy) & np.isfinite(y_temp)
            if mask.sum() < 10:
                print(f"  ⚠ {base}: insufficient valid data after NaN filtering.")
                continue

            X = x_proxy[mask].reshape(-1, 1)
            y = y_temp[mask]

            model = LinearRegression().fit(X, y)
            slope = float(model.coef_[0])
            intercept = float(model.intercept_)

            stats = _reg_stats(X.flatten(), y, f"{base} Final")
            self.final_equations[base] = {
                'slope': slope,
                'intercept': intercept,
                'stats': stats
            }

            print(f"  {base} Final Model: Temp = {slope:.4f}*{base} + {intercept:.4f}")
            print(f"    R²={stats['r2']:.3f}, RMSE={stats['rmse']:.3f}, n={stats['n']}\n")


    # ---------- BLOCK BOOTSTRAP ----------
    def block_bootstrap_calibration(self, proxy='Mg/Sr', block_len=7, n_boot=1000):
        print("="*60)
        print(f"BLOCK BOOTSTRAP (proxy={proxy}, block_len={block_len}, n_boot={n_boot})")
        print("="*60)

        if proxy not in self.composite_data:
            print(f"❌ Composite data for {proxy} not found.\n")
            return

        comp = self.composite_data[proxy]
        common_dates = comp.index.intersection(self.temp_full.index)
        x = comp.loc[common_dates, 'mean'].values
        y = self.temp_full.loc[common_dates].values
        n = len(x)
        if n < 20:
            print("❌ Too few days for meaningful bootstrap.\n")
            return

        slopes, intercepts, rmses = [], [], []
        starts = np.arange(0, n - block_len + 1)

        for _ in range(n_boot):
            idx = []
            while len(idx) < n:
                s = np.random.choice(starts)
                idx.extend(range(s, s + block_len))
            idx = np.array(idx[:n])

            xb = x[idx]
            yb = y[idx]
            msk = np.isfinite(xb) & np.isfinite(yb)
            if msk.sum() < 10:
                continue

            Xb = xb[msk].reshape(-1, 1)
            yb = yb[msk]

            model = LinearRegression().fit(Xb, yb)
            slope = float(model.coef_[0])
            intercept = float(model.intercept_)
            yhat = model.predict(Xb)
            rmse = np.sqrt(mean_squared_error(yb, yhat))

            slopes.append(slope)
            intercepts.append(intercept)
            rmses.append(rmse)

        slopes = np.array(slopes)
        intercepts = np.array(intercepts)
        rmses = np.array(rmses)

        out = {
            'slope_ci': np.percentile(slopes, [2.5, 50, 97.5]),
            'intercept_ci': np.percentile(intercepts, [2.5, 50, 97.5]),
            'rmse_ci': np.percentile(rmses, [2.5, 50, 97.5]),
            'n_boot': len(slopes)
        }
        self.bootstrap_results[proxy] = out

        print(f"  Slope 95% CI: {out['slope_ci'][0]:.4f} – {out['slope_ci'][2]:.4f}")
        print(f"  RMSE 95% CI: {out['rmse_ci'][0]:.3f} – {out['rmse_ci'][2]:.3f}")
        print(f"  Effective bootstraps: {out['n_boot']}\n")
        return out

      # ---------- STEP 7: PLOTTING ----------
    def plot_results(self):
        print("="*60)
        print("STEP 7: PLOTTING")
        print("="*60)

        proxies = ['Mg/Sr', 'Mg/Ca', 'Sr/Ca']
        fig, axes = plt.subplots(4, 1, figsize=(16, 22), sharex=True) # Slightly taller fig

        temp_std = self.temp_full.rolling(window=7, center=True).std()
        temp_std = temp_std.fillna(method='bfill').fillna(method='ffill')

        df_mgsr = self.aligned_data['Mg/Sr']
        branches = sorted(df_mgsr['Branch'].unique())
        b_colors = {b: plt.cm.tab10(i / max(1, len(branches)-1)) for i, b in enumerate(branches)}

        calib_start = pd.to_datetime(self.config['dates']['deploy_start'])

        def add_bg(ax):
            ax2 = ax.twinx()
            ax2.plot(self.temp_full.index, self.temp_full, 'k:', alpha=0.4, lw=1.2,
                     label='Logger mean')
            upper = self.temp_full + temp_std
            lower = self.temp_full - temp_std
            ax2.fill_between(self.temp_full.index, lower, upper,
                             color='lightgray', alpha=0.25, label='Logger ±1σ')
            ax2.set_ylabel("Temperature (°C)", fontweight='bold', color='dimgray', fontsize=14)
            ax2.tick_params(axis='y', labelcolor='dimgray', labelsize=12)

        # Panels A–C
        for i, proxy in enumerate(proxies):
            ax = axes[i]
            add_bg(ax)

            if proxy not in self.aligned_data or proxy not in self.composite_data:
                continue

            df = self.aligned_data[proxy]
            df_c = df[df['Date'] >= calib_start]

            for b, g in df_c.groupby('Branch'):
                g_sorted = g.sort_values('Date')
                ax.plot(g_sorted['Date'], g_sorted['Value'], 'o-',
                        color=b_colors[b], markersize=3, alpha=0.5, lw=0.8,
                        label=b if i == 0 else "")

            comp = self.composite_data[proxy]
            comp = comp[comp.index >= calib_start]
            ax.plot(comp.index, comp['mean'], 'k-', lw=3, alpha=0.9,
                    label='Composite mean' if i == 0 else "")
            ax.fill_between(comp.index,
                            comp['mean'] - comp['std'],
                            comp['mean'] + comp['std'],
                            color=COLORS.get(proxy, 'k'), alpha=0.25)

            centroids = df_c.groupby('Branch')['Value'].mean()
            x_centroid = comp.index[-1] + pd.Timedelta(days=3)
            for b, v in centroids.items():
                ax.scatter(x_centroid, v, color=b_colors[b], edgecolor='k',
                           s=45, zorder=5)

            ax.set_ylabel(proxy, fontweight='bold', color=COLORS.get(proxy, 'k'), fontsize=16)
            ax.tick_params(axis='y', labelsize=12)

            ax.text(0.02, 0.9, chr(65 + i), transform=ax.transAxes,
                    fontweight='bold', fontsize=18,
                    bbox=dict(facecolor='white', alpha=0.8, boxstyle='round'))

            if i == 0:
                ax.legend(loc='upper center', ncol=min(4, len(branches)+1),
                          bbox_to_anchor=(0.5, 1.22), fontsize=11, framealpha=0.9)

        # Panel D: Reconstructions
        ax = axes[3]

        # 1. ENVIRONMENTAL CONTEXT
        ax.fill_between(self.temp_full.index,
                        self.temp_min,
                        self.temp_max,
                        color='gray', alpha=0.15, label='In-situ Daily Range')

        ax.plot(self.temp_full.index, self.temp_full, 'k--', lw=1.5, alpha=0.5,
                label='Logger Daily Mean')

        # 2. PROXY RECONSTRUCTIONS
        for proxy in proxies:
            if proxy not in self.final_equations or proxy not in self.composite_data:
                continue

            comp = self.composite_data[proxy]
            comp = comp[comp.index >= calib_start]
            eq = self.final_equations[proxy]

            rec_mean = comp['mean'].values * eq['slope'] + eq['intercept']
            color = COLORS.get(proxy, 'k')

            ax.plot(comp.index, rec_mean, 'o-', markersize=4, lw=2.0,
                    alpha=0.9, color=color, label=f'{proxy} Rec')

        # 3. AESTHETICS
        for spine in ax.spines.values():
            spine.set_visible(True)
            spine.set_color('black')
            spine.set_linewidth(1.2)
        ax.grid(False)

        ax.set_ylabel("Temperature (°C)", fontweight='bold', fontsize=16)
        ax.tick_params(axis='both', labelsize=12)

        ax.legend(loc='upper left', fontsize=11, framealpha=1.0,
                 edgecolor='black', fancybox=False, ncol=1)

        ax.text(0.02, 0.9, 'D', transform=ax.transAxes,
                fontweight='bold', fontsize=18,
                bbox=dict(facecolor='white', edgecolor='black', pad=4.0))

        axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))
        plt.xticks(rotation=0, ha='center', fontsize=12)

        # Stats Box
        lines = []
        for proxy in proxies:
            if proxy in self.final_equations:
                s = self.final_equations[proxy]['stats']
                lines.append(f"{proxy}: $R^2$={s['r2']:.2f}, RMSE={s['rmse']:.2f}°C")

        if lines:
            fig.text(0.5, 0.03, "  |  ".join(lines), ha='center',
                     bbox=dict(facecolor='white', edgecolor='black',
                             boxstyle='square', pad=0.6),
                     fontsize=12)

        plt.tight_layout()
        plt.subplots_adjust(bottom=0.1, top=0.95)
        plt.show()

    # ---------- STEP 8: SEPARATE BRANCH PLOTS (BIGGER FONTS) ----------
    def plot_individual_branch_reconstructions(self):
        print("\n" + "="*70)
        print("STEP 8: INDIVIDUAL BRANCH VALIDATION (Mg/Sr)")
        print("="*70)

        proxy = 'Mg/Sr'
        if proxy not in self.aligned_data or proxy not in self.final_equations:
            print("❌ Mg/Sr data or equation missing.")
            return

        df = self.aligned_data[proxy]
        eq = self.final_equations[proxy]
        branches = sorted(df['Branch'].unique())

        fig, axes = plt.subplots(len(branches), 1, figsize=(12, 20), sharex=True, sharey=True)
        if len(branches) == 1: axes = [axes]

        for i, branch in enumerate(branches):
            ax = axes[i]

            branch_data = df[df['Branch'] == branch].sort_values('Date')

            if 'Value' not in branch_data.columns:
                 print(f"❌ Column 'Value' not found.")
                 return

            temp_rec = branch_data['Value'].values * eq['slope'] + eq['intercept']
            dates = branch_data['Date']

            logger_at_dates = self.temp_full.reindex(dates, method='nearest').values
            stats = _reg_stats(temp_rec, logger_at_dates, f"{branch}")

            ax.fill_between(self.temp_full.index, self.temp_min, self.temp_max,
                            color='gray', alpha=0.15, label='In-situ Range')
            ax.plot(self.temp_full.index, self.temp_full, 'k--', lw=1.2, alpha=0.5)

            color = COLORS.get('Mg/Sr', 'purple')

            ax.plot(dates, temp_rec, '-', lw=1.2, color=color, alpha=0.6)
            ax.scatter(dates, temp_rec, s=25, color=color, edgecolors='white',
                       linewidth=0.6, zorder=3, label=f'{branch} Data')

            stat_text = f"{branch}: $R^2$={stats['r2']:.2f}, RMSE={stats['rmse']:.2f}°C"
            ax.text(0.02, 0.88, stat_text, transform=ax.transAxes,
                    fontweight='bold', fontsize=12,
                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='square,pad=0.5'))

            ax.grid(False)
            for spine in ax.spines.values():
                spine.set_visible(True)
                spine.set_color('black')
                spine.set_linewidth(1.2)

            ax.set_ylabel("Temp (°C)", fontsize=12, fontweight='bold')
            ax.tick_params(axis='both', labelsize=12)

            if i == 0:
                ax.legend(loc='upper right', fontsize=10, framealpha=1.0, edgecolor='black')

        axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))
        plt.xticks(rotation=0, ha='center', fontsize=12)
        plt.xlabel("Date (2024)", fontweight='bold', fontsize=14)
        plt.tight_layout()
        plt.subplots_adjust(top=0.96)
        plt.show()

pipeline = RhodolithPipeline(CONFIG)
pipeline.authenticate()
pipeline.load_temperature_data()
pipeline.load_curve6_curve7()
pipeline.screen_best_linear_branch()
pipeline.generate_synthetic_master()

# Test Sakoe–Chiba windows
for w in [15, 30]:
    print("\n" + "-"*50)
    print("Running DTW with window_days =", w)
    pipeline.perform_dtw_alignment(window_days=w)
    pipeline.build_composite_and_calibrate()
    stats = pipeline.final_equations['Mg/Sr']['stats']
    print(f"Window {w}: Mg/Sr R² = {stats['r2']:.3f}, RMSE = {stats['rmse']:.3f} °C")

# Choose which window you want for the final plots (e.g. 15 days)
pipeline.perform_dtw_alignment(window_days=30)
pipeline.build_composite_and_calibrate()
pipeline.block_bootstrap_calibration(proxy='Mg/Sr', block_len=7, n_boot=1000)
pipeline.plot_results()
pipeline.plot_individual_branch_reconstructions()

# ---------- STEP 9: MULTI-ELEMENT HEATMAP (Formatted Isotope Labels) ----------
def plot_element_correlations_heatmap(self):
    print("\n" + "="*70)
    print("STEP 9: MULTI-ELEMENT CORRELATION HEATMAP (Formatted)")
    print("="*70)

    import re  # Import re inside method or at top of file
    import seaborn as sns
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    from scipy.stats import pearsonr

    if self.synthetic_master is None:
        print("❌ Master missing.")
        return

    master_vals = self.synthetic_master['MgSr_Target'].values
    master_dates = self.synthetic_master['Date'].values
    window = int(self.config.get('dtw', {}).get('window', 10) or 10)
    psi = self.config['dtw']['psi']

    results = []

    # 1. Helper function for isotope notation (e.g. Li7/Ca43 -> ^{7}Li/^{43}Ca)
    def format_isotope_label(label):
        label = label.replace("Mean ", "").strip()
        # Remove units for cleaner plot labels, or keep them if preferred
        # match_units = re.search(r"\((.*?)\)", label)
        # units = f" ({match_units.group(1)})" if match_units else ""
        label = re.sub(r"\(.*\)", "", label).strip()

        parts = label.split('/')
        formatted_parts = []
        for part in parts:
            # Look for Element + Mass (e.g., Li7)
            # This regex assumes Element first, then digits (Li7)
            m = re.match(r"([A-Za-z]+)(\d+)", part)
            if m:
                elem, mass = m.groups()
                formatted = f"$^{{{mass}}}\\mathrm{{{elem}}}$"
                formatted_parts.append(formatted)
            else:
                # If no mass number, just wrap element in roman font
                formatted_parts.append(f"$\\mathrm{{{part}}}$")
        return '/'.join(formatted_parts)

    # 2. Gather Data
    for i in range(1, 8):
        df_raw, name = self.get_raw_branch_data(i)
        if df_raw is None:
            continue
        df = df_raw.apply(pd.to_numeric, errors='coerce')

        sr_col = _find_col(df, ('ca',), ('sr',))
        mg_col = _find_col(df, ('ca',), ('mg',))
        if not sr_col or not mg_col:
            continue

        mgsr_raw = df[mg_col] / df[sr_col]
        valid_mask = mgsr_raw.notna()
        mgsr_clean = mgsr_raw[valid_mask].values

        try:
            path = dtw.warping_path(mgsr_clean, master_vals, window=window, psi=psi)
            q_idx = [p[0] for p in path]
            m_idx = [p[1] for p in path]

            aligned_dates = master_dates[m_idx]
            aligned_temps = self.temp_full.reindex(aligned_dates, method='nearest').values

            # Identify Ratio Columns
            ratio_cols = [c for c in df.columns if '/' in c and 'mmol' in c]
            df_clean = df.loc[valid_mask].iloc[q_idx]

            for col in ratio_cols:
                # Use the full raw column name initially, we format it later
                vals = df_clean[col].values
                mask_corr = np.isfinite(vals) & np.isfinite(aligned_temps)

                if mask_corr.sum() > 20 and np.std(vals[mask_corr]) > 0:
                    r, p = pearsonr(vals[mask_corr], aligned_temps[mask_corr])
                    results.append({
                        'Branch': name,
                        'Element_Ratio': col,  # Store raw name first
                        'Correlation': r,
                        'p_value': p
                    })
        except Exception:
            continue

    if not results:
        print("❌ No correlations found.")
        return

    res_df = pd.DataFrame(results)

    # 3. Deduplicate (Average isotopes if multiple cols mapped to same ratio)
    # First, apply formatting to create a common key
    res_df['Formatted_Label'] = res_df['Element_Ratio'].apply(format_isotope_label)

    # Now groupby Branch and Formatted_Label
    grouped = res_df.groupby(['Branch', 'Formatted_Label']).agg({
        'Correlation': 'mean',
        'p_value': 'mean'  # Geometric mean might be better for p-values, but arithmetic is fine for visualization
    }).reset_index()

    # 4. Create Pivot Tables
    # Create annotation: "0.85\n(p=0.001)"
    grouped['annot'] = grouped.apply(
        lambda row: f"R = {row['Correlation']:.2f}\n($p$ = {row['p_value']:.3f})",
        axis=1
    )

    heatmap_r = grouped.pivot(index='Formatted_Label', columns='Branch', values='Correlation')
    heatmap_annot = grouped.pivot(index='Formatted_Label', columns='Branch', values='annot')

    # 5. Sort Rows (Optional: Put Mg/Sr at top)
    # We can sort by mean correlation to make it look organized
    mean_corr = heatmap_r.mean(axis=1).sort_values(ascending=False)
    heatmap_r = heatmap_r.reindex(mean_corr.index)
    heatmap_annot = heatmap_annot.reindex(mean_corr.index)

    # 6. Plot
    plt.figure(figsize=(10, len(heatmap_r) * 0.6 + 2))

    # cmap='RdBu_r' -> Red is positive (high), Blue is negative (low)
    # vmin=-1, vmax=1 centers white at 0
    ax = sns.heatmap(
        heatmap_r,
        annot=heatmap_annot,
        fmt='',
        center=0, vmin=-1, vmax=1, cmap='RdBu_r',
        linewidths=0.5, linecolor='white',
        cbar_kws={'label': 'Pearson Correlation (R)'}
    )

    # Y-labels are already formatted, just ensure rotation
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)

    plt.title('Correlation of Element Ratios with Temperature\n(Aligned via Mg/Sr)',
              fontweight='bold', pad=15)
    plt.xlabel(None)
    plt.ylabel(None)
    plt.tight_layout()
    plt.show()


# Run it
pipeline.plot_element_correlations_heatmap = plot_element_correlations_heatmap.__get__(pipeline)
pipeline.plot_element_correlations_heatmap()

# ---------- STEP 9: MULTI-ELEMENT HEATMAP (Robust) ----------
    def plot_element_correlations_heatmap(self):
        print("\n" + "="*70)
        print("STEP 9: MULTI-ELEMENT CORRELATION HEATMAP")
        print("="*70)

        if self.synthetic_master is None:
            print("❌ Synthetic master missing.")
            return

        master_vals = self.synthetic_master['MgSr_Target'].values
        master_dates = self.synthetic_master['Date'].values
        window = int(self.config.get('dtw', {}).get('window', 10) or 10)
        psi = self.config['dtw']['psi']

        results = []

        for i in range(1, 8):
            df_raw, name = self.get_raw_branch_data(i)
            if df_raw is None: continue

            df = df_raw.apply(pd.to_numeric, errors='coerce')

            # Find Mg/Sr to drive alignment
            sr_col = _find_col(df, ('ca',), ('sr',))
            mg_col = _find_col(df, ('ca',), ('mg',))
            if not sr_col or not mg_col: continue

            mgsr_raw = df[mg_col] / df[sr_col]
            valid_mask = mgsr_raw.notna()
            mgsr_clean = mgsr_raw[valid_mask].values

            # DTW Alignment
            try:
                path = dtw.warping_path(mgsr_clean, master_vals, window=window, psi=psi)
                q_idx = [p[0] for p in path]
                m_idx = [p[1] for p in path]

                aligned_dates = master_dates[m_idx]
                aligned_temps = self.temp_full.reindex(aligned_dates, method='nearest').values

                # Identify Ratio Columns
                ratio_cols = [c for c in df.columns if '/' in c and 'mmol' in c]

                # Process each ratio
                df_clean = df.loc[valid_mask].iloc[q_idx]

                for col in ratio_cols:
                    # Robust Name Cleaning
                    # 1. Remove units
                    base = col.split('(')[0].strip()
                    # 2. Remove digits (Isotopes) e.g. Mg25 -> Mg
                    clean_el = ''.join([c for c in base if not c.isdigit()])
                    # 3. Handle duplicate markers like .1
                    if '.' in clean_el: clean_el = clean_el.split('.')[0]

                    short_name = clean_el

                    vals = df_clean[col].values
                    mask_corr = np.isfinite(vals) & np.isfinite(aligned_temps)

                    if mask_corr.sum() > 10:
                        r, p = pearsonr(vals[mask_corr], aligned_temps[mask_corr])
                        results.append({
                            'Branch': name,
                            'Element': short_name,
                            'R': r,
                            'P': p
                        })
            except Exception as e:
                pass

        if not results:
            print("❌ No correlations.")
            return

        res_df = pd.DataFrame(results)

        # --- FIX: DEDUPLICATE BEFORE PIVOT ---
        # If multiple columns mapped to "Mg/Ca" (e.g. Mg24 and Mg25), average their R values
        res_df = res_df.groupby(['Branch', 'Element']).agg({'R': 'mean', 'P': 'mean'}).reset_index()

        # Pivot
        heatmap_r = res_df.pivot(index='Branch', columns='Element', values='R')
        heatmap_p = res_df.pivot(index='Branch', columns='Element', values='P')

        # Sorting
        cols = sorted(heatmap_r.columns)
        priority = ['Mg/Sr', 'Mg/Ca', 'Sr/Ca', 'Li/Ca', 'B/Ca', 'Ba/Ca', 'U/Ca']
        sorted_cols = [c for c in priority if c in cols] + [c for c in cols if c not in priority]
        heatmap_r = heatmap_r[sorted_cols]
        heatmap_p = heatmap_p[sorted_cols]

        # Plot
        plt.figure(figsize=(12, len(heatmap_r)*0.8 + 2))

        annot_labels = []
        for i in range(heatmap_r.shape[0]):
            row = []
            for j in range(heatmap_r.shape[1]):
                r_val = heatmap_r.iloc[i, j]
                p_val = heatmap_p.iloc[i, j]
                if np.isnan(r_val):
                    row.append("")
                else:
                    stars = ""
                    if p_val < 0.001: stars = "***"
                    elif p_val < 0.01: stars = "**"
                    elif p_val < 0.05: stars = "*"
                    row.append(f"{r_val:.2f}\n{stars}")
            annot_labels.append(row)

        sns.heatmap(heatmap_r, annot=np.array(annot_labels), fmt="",
                    cmap="RdBu_r", center=0, vmin=-1, vmax=1,
                    linewidths=1, linecolor='white',
                    cbar_kws={'label': 'Pearson R vs Temp', 'shrink': 0.8})

        plt.title("Element/Ca vs. Temperature Correlations\n(Aligned via Mg/Sr)", fontweight='bold', pad=15)
        plt.tight_layout()
        plt.show()

pipeline.plot_element_correlations_heatmap = plot_element_correlations_heatmap.__get__(pipeline)
pipeline.plot_element_correlations_heatmap()

# ---------- STEP 11: SCREENING ALL ELEMENTS PER BRANCH (ROBUST) ----------
    def screen_all_elements_per_branch(self):
        print("\n" + "="*70)
        print("STEP 11: SCREENING ALL ELEMENTS PER BRANCH (Mg/Sr Aligned)")
        print("="*70)

        if self.synthetic_master is None:
            print("❌ Master missing.")
            return

        master_vals = self.synthetic_master['MgSr_Target'].values
        master_dates = self.synthetic_master['Date'].values
        window = int(self.config.get('dtw', {}).get('window', 10) or 10)
        psi = self.config['dtw']['psi']

        for i in range(1, 8):
            df_raw, name = self.get_raw_branch_data(i)
            if df_raw is None: continue

            print(f"\n--- {name} ---")
            df = df_raw.apply(pd.to_numeric, errors='coerce')

            # Drive Alignment
            sr_col = _find_col(df, ('ca',), ('sr',))
            mg_col = _find_col(df, ('ca',), ('mg',))
            if not sr_col or not mg_col: continue

            mgsr_raw = df[mg_col] / df[sr_col]
            valid_mask = mgsr_raw.notna()
            mgsr_clean = mgsr_raw[valid_mask].values

            try:
                # Alignment
                path = dtw.warping_path(mgsr_clean, master_vals, window=window, psi=psi)
                q_idx = [p[0] for p in path]
                m_idx = [p[1] for p in path]

                aligned_temps = self.temp_full.reindex(master_dates[m_idx], method='nearest').values

                ratio_cols = [c for c in df.columns if '/' in c and 'mmol' in c]
                branch_results = []

                df_subset = df.loc[valid_mask].iloc[q_idx]

                for col in ratio_cols:
                    base = col.split('(')[0].strip()
                    clean_el = ''.join([c for c in base if not c.isdigit()])
                    if '.' in clean_el: clean_el = clean_el.split('.')[0]

                    vals = df_subset[col].values

                    # --- ROBUSTNESS CHECKS ---
                    mask = np.isfinite(vals) & np.isfinite(aligned_temps)

                    # 1. Not enough data?
                    if mask.sum() < 20:
                        continue

                    # 2. Zero Variance? (All values identical)
                    if np.std(vals[mask]) == 0 or np.std(aligned_temps[mask]) == 0:
                        continue

                    # Run Regression
                    slope, intercept, r_val, p_val, std_err = linregress(vals[mask], aligned_temps[mask])
                    r2 = r_val**2

                    branch_results.append({
                        'Element': clean_el,
                        'R2': r2,
                        'Slope': slope,
                        'n': mask.sum()
                    })

                if branch_results:
                    res_df = pd.DataFrame(branch_results)
                    res_df = res_df.groupby('Element').agg({'R2':'mean', 'Slope':'mean', 'n':'mean'}).reset_index()
                    res_df = res_df.sort_values('R2', ascending=False)
                    print(res_df.to_string(index=False, float_format="%.3f"))
                else:
                    print("No valid regressions found.")

            except Exception as e:
                print(f"Skipped {name} due to alignment error: {e}")

from scipy.stats import linregress
pipeline.screen_all_elements_per_branch = screen_all_elements_per_branch.__get__(pipeline)
pipeline.screen_all_elements_per_branch()

def permutation_test_mgsr(pipeline, n_perm=200, block_len=7, random_state=42):
    """
    Permute logger temperature relative to the Mg/Sr composite while
    preserving autocorrelation, refit T ~ Mg/Sr, and record R².

    Requires that pipeline.composite_data['Mg/Sr'] and pipeline.temp_full exist.
    """
    rng = np.random.default_rng(random_state)

    comp = pipeline.composite_data['Mg/Sr']
    # Use all overlapping dates (same as calibration)
    common_dates = comp.index.intersection(pipeline.temp_full.index)
    x = comp.loc[common_dates, 'mean'].values
    y = pipeline.temp_full.loc[common_dates].values
    n = len(x)

    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import r2_score

    # In-sample R² for reference
    msk = np.isfinite(x) & np.isfinite(y)
    X = x[msk].reshape(-1, 1)
    y_clean = y[msk]
    base_model = LinearRegression().fit(X, y_clean)
    base_r2 = r2_score(y_clean, base_model.predict(X))
    print(f"Observed Mg/Sr R² = {base_r2:.3f} (n={len(y_clean)})")

    perm_r2_shift = []
    perm_r2_block = []

    # Helper: compute R² after permuting y_perm against same x
    def _fit_r2(y_perm):
        m = np.isfinite(x) & np.isfinite(y_perm)
        if m.sum() < 10:
            return np.nan
        Xp = x[m].reshape(-1, 1)
        yp = y_perm[m]
        model = LinearRegression().fit(Xp, yp)
        return r2_score(yp, model.predict(Xp))

    # 1a. Circular shifts of temperature
    for _ in range(n_perm):
        k = rng.integers(1, n)  # non-zero shift
        y_shift = np.roll(y, k)
        perm_r2_shift.append(_fit_r2(y_shift))

    # 1b. Block permutations of temperature
    starts = np.arange(0, n, block_len)
    for _ in range(n_perm):
        # Make blocks (last one truncated if necessary)
        blocks = [slice(s, min(s + block_len, n)) for s in starts]
        rng.shuffle(blocks)
        idx = np.concatenate([np.arange(b.start, b.stop) for b in blocks])
        y_block = y[idx]
        perm_r2_block.append(_fit_r2(y_block))

    perm_r2_shift = np.array(perm_r2_shift)
    perm_r2_block = np.array(perm_r2_block)

    print("\nPermutation test (circular shifts):")
    print(f"  Median null R² = {np.nanmedian(perm_r2_shift):.3f}")
    print(f"  95th percentile null R² = {np.nanpercentile(perm_r2_shift, 95):.3f}")
    print(f"  Proportion null R² >= observed = "
          f"{np.mean(perm_r2_shift >= base_r2):.3f}")

    print("\nPermutation test (block permutations):")
    print(f"  Median null R² = {np.nanmedian(perm_r2_block):.3f}")
    print(f"  95th percentile null R² = {np.nanpercentile(perm_r2_block, 95):.3f}")
    print(f"  Proportion null R² >= observed = "
          f"{np.mean(perm_r2_block >= base_r2):.3f}")

    return {
        "observed_r2": base_r2,
        "shift_r2": perm_r2_shift,
        "block_r2": perm_r2_block,
    }

# Run after your normal pipeline
perm_results = permutation_test_mgsr(pipeline, n_perm=500, block_len=7)

def proxy_only_null_flat_master(pipeline, window_days=None):
    """
    Replace the temperature-based synthetic master by a flat Mg/Sr master
    (constant equal to the mean Mg/Sr in the current master) and recompute
    DTW + composite + calibration. Returns Mg/Sr R² under the null.
    """
    # Backup original master
    master_backup = pipeline.synthetic_master.copy()

    # Build flat master with same dates, constant value
    mean_mgsr = master_backup['MgSr_Target'].mean()
    flat_master = master_backup.copy()
    flat_master['MgSr_Target'] = mean_mgsr
    pipeline.synthetic_master = flat_master

    print("\n=== PROXY-ONLY NULL: FLAT MASTER ===")
    pipeline.perform_dtw_alignment(window_days=window_days)
    pipeline.build_composite_and_calibrate()

    r2_null = pipeline.final_equations['Mg/Sr']['stats']['r2']
    print(f"Flat-master Mg/Sr R² = {r2_null:.3f}")

    # Restore original master
    pipeline.synthetic_master = master_backup
    return r2_null

r2_flat = proxy_only_null_flat_master(pipeline, window_days=30)

